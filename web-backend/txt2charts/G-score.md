生成性任务的评价一直具有挑战性。在过去的文献摘要研究中，主要依赖于文本摘要指标，如ROUGE（Lin，2004）。然而，传统的文本摘要评价指标，如ROUGE，无法完全衡量文献摘要的质量。因此，采用更全面的评价标准在各个方面都变得至关重要，以确保生成的文献摘要符合必要的标准。

受到G-Eval（Liu等，2023年）的启发，我们尝试使用LLM对文献摘要进行评估。基于对文献摘要的研究，我们建立了自动评价的六维指标（Justitia和Wang，2022）。评估步骤如下：我们使用大型语言模型（LLM）对生成的摘要在六个维度上进行评分，并从一系列模型生成的摘要中进行投票，选出最佳摘要。

特别地，为了确保评估的公平性和一致性，我们在一次对话中同时对多个模型生成的结果进行评分和投票。具体的评价标准如下：

1. 一致性（1-5分）：生成的摘要与黄金摘要内容一致。生成的摘要不能包含与黄金摘要冲突的内容。
2. 连贯性（1-5分）：生成的摘要中的语言连贯性质量。摘要不应该只是一堆相关信息的堆砌。
3. 比较（1-5分）：评估生成的摘要是否对参考文献和相关工作进行了比较分析。是否提供了类似相关作品的综合总结。
4. 完整性（1-5分）：评估摘要是否涵盖了基本要素，包括研究背景、参考文献摘要、过去的研究评价、贡献和创新。
5. 流利程度（1-5分）：从语法、拼写、标点符号、用词和句子结构等方面评估摘要的质量。
6. 引用准确性（1-5分）：评估摘要在提及参考文献时是否正确地以"[Reference i]"格式引用了参考文献。

通过以上评价标准，我们可以更全面地评估生成的文献摘要的质量和准确性。